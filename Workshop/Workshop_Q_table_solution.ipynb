
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome \n",
    "Welcome to this workshop!\n",
    "\n",
    "The workshop is structured such that for each cell, you will write your own code and the code will then be asserted in the next cell. The assertion cell is marked \"# Do not edit - Assertion cell #\". <br>\n",
    "The code should be written between \"Write code below\" and \"Write code above\". If your code does not pass the assertion, you will have to rewrite it before continuing.\n",
    "\n",
    "### Task:\n",
    "Implement the method multiply_input_by_2\n",
    "- The code should take the input variabel \"input_variabel\", and multiply it by 2\n",
    "- The answer should be stored in the variabel \"result\"\n",
    "- When the method is implemented, run the cell.\n",
    "\n",
    "Hotkey to run a cell: CTRL + ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(input_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    result = input_variabel * 2\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\"\n",
    "print(\"Great, you correctly implemented the method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages\n",
    "To implement the native Q-learning algorithm and use it in an environment, we just need two additional packages.\n",
    "\n",
    "_Numpy_\n",
    "- Numpy adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. We could do this workshop using native Python arrays and built-in methods, however using Numpy simplifies the process. \n",
    "- To call the methods in the package, we use the \"as np\" command. Such that, to create a numpy table with 2 rows and 2 columns where all the cells are zero, you can simply write np.seros(2,2).\n",
    "\n",
    "_OpenAI Gym_\n",
    "- The Gym library is a collection of demo problems an associated environment, that you can use to work out your reinforcement learning algorithms. These range from simple text problems, to complex physical problems, to Atari video games. OpenAI Gym is therefore by many the preferred framework to learn and test Reinforcement learning algorithms.\"\n",
    "\n",
    "\n",
    "### Task:\n",
    "To import the necessary packages, simply run the cell below, and then run the assertion cell to verify that they have been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(gym)\n",
    "    print(\"Great, the packages were imported correctly!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Introduction\n",
    "\n",
    "The environment we are going to use is a simple grid world, where the agent is controlling a taxi. The environment was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop the passenger off in another. You can read more about the environment [here](https://gym.openai.com/envs/Taxi-v2/).\n",
    "\n",
    "### Task:\n",
    "Create the environment variable containing all necessary methods to run the Taxi-v3 game. <br>\n",
    "\n",
    "_Tip: Just run the cell below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_id = \"Taxi-v3\"\n",
    "env = gym.make(environment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(env)\n",
    "    print(\"You successfully created the environement {}\".format(env.spec.id))\n",
    "except:\n",
    "    print(\"You create the environment incorrectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the workshop. \n",
    "\n",
    "### Task\n",
    "Do as you have done before, simply mark the cell below and press CTRL + Enter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.reset()\n",
    "        \n",
    "    def play(self, action):\n",
    "        if not (0 <= int(action) <= 5):\n",
    "            print(\"Action value must either be 0,1,2,3,4 or 5\")\n",
    "            return\n",
    "\n",
    "        clear_output()\n",
    "        _, reward, done, _ = self.env.step(action)\n",
    "        self.env.render()\n",
    "        print(\"Reward: \", reward)\n",
    "        \n",
    "        if(done):\n",
    "            print(\"Game completed, resetting environment!\")\n",
    "            self.reset_env()\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        print(\"Environment has been reset\")\n",
    "        time.sleep(3)\n",
    "        clear_output()\n",
    "\n",
    "class MockData():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.seed(10)\n",
    "        self.should_assert = True\n",
    "        \n",
    "    def get_Q(self):\n",
    "        Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q[0:2,0] = 10\n",
    "        return Q\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "    \n",
    "    def turn_off_assertion(self):\n",
    "        self.should_assert = False\n",
    "        \n",
    "    def do_assertion(self):\n",
    "        return self.should_assert\n",
    "\n",
    "def visualize_q_table():\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "    heat_map = sb.heatmap(Q_trained)\n",
    "    plt.show()\n",
    "\n",
    "def plot_visualization(data):\n",
    "    \n",
    "    reward_list = data[0]\n",
    "    iteration_list = data[1]\n",
    "    epsilon_list = data[2]\n",
    "    \n",
    "    episodes = range(len(reward_list))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(311)\n",
    "    ax.set_title(\"Reward\")\n",
    "    ax = plt.plot(episodes, reward_list)\n",
    "\n",
    "    ax = plt.subplot(312)\n",
    "    ax.set_title(\"Iterations per episode\")\n",
    "    ax.plot(episodes, iteration_list)\n",
    "    \n",
    "    ax = plt.subplot(313)\n",
    "    ax.set_title(\"Epsilon\")\n",
    "    ax.set_xlabel(\"Episodes\")\n",
    "    ax.plot(episodes, epsilon_list)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def reset_env_and_update_params(env):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    return state, done, iterations, total_reward\n",
    "\n",
    "def render_performance(env, Q):\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    t_sleep = 1.2\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    time.sleep(t_sleep)\n",
    "    clear_output()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state,:]) \n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        state =  new_state\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(t_sleep)\n",
    "        clear_output()\n",
    "        \n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        if (iterations >= 20): \n",
    "            done = True\n",
    "            print(\"Agent did not complete the episode within 20 iterations, train your agent better!\")\n",
    "            return\n",
    "    \n",
    "    print(\"Your agent completed the task using {} iterations, \\\n",
    "          and got a total reward of {}\".format(iterations, total_reward))\n",
    "    \n",
    "test_env = TestEnvironment()\n",
    "mock = MockData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(mock)\n",
    "    print(\"Great, the support methods were created!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the environment\n",
    "Before we start creating the Q-learning agent, we will explore the environment we are going to use. This is done using the support methods we have created for you.\n",
    "\n",
    "**How to play**\n",
    "1. Simply input an action, e.g. the integer 1, where it is indicated in the cell below, and click CTRL + Enter. \n",
    "2. The game will reset when you have picked up and then dropped off the passenger at the indicated location.\n",
    "3. If you would like to reset the test environment during the execution, run the cell which indicates this below. \n",
    "\n",
    "*Tip*: The bold blue colored letter shows where the passenger should be picked up, and the pink letter indicates the drop-off spot.\n",
    "\n",
    "**Possible actions**\n",
    "0. Move down\n",
    "1. Move up\n",
    "2. Move right\n",
    "3. Move left\n",
    "4. Pick up passenger\n",
    "5. Drop off passenger\n",
    "\n",
    "### Task:\n",
    "Play around with the environment to understand the rewards and how the game dynamics work. When you feel confident with the environment,  move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to reset the environment\n",
    "test_env.reset_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to play the game\n",
    "\n",
    "# Input your action below\n",
    "action = 0\n",
    "# Input your action above\n",
    "\n",
    "test_env.play(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion point\n",
    "The game above is a simple environment, which is easy for humans to solve. However, if you were to use traditional programming, how would you solve it? \n",
    "\n",
    "### Task\n",
    "Use 5 minutes in the group to come up with a strategy, and create a quick draft!\n",
    "\n",
    "<img src=\"Images/Discussion.jpg\" alt=\"drawing\" width=\"400\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "In Reinforcement learning an agent moves from a state $S_{t}$, to a new state $S_{t+1}$ by taking an action $A_{t}$, and receiving the reward $R_{t}$ as illustrated below. The agent will repeat this process for a defined amount of iterations, and this process as a whole is known as an episode. The goal of the Reinforcement learning agent is to maximize the total reward it can collect during an episode. To improve the performance, it learns from each state transition, i.e. move from $S_{t}$ to $S_{t+1}$. How it learns, is what separates the different Reinforcement learning algorithms.\n",
    "\n",
    "<img src=\"Images/Agent.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "In this notebook, we will work with one of the most iconic Reinforcement learning algorithms in its simplest form, namely Q-learning using Q-tables. The Q-tables holds the values of the strategy that gives us the highest reward,  according to the agent's knowledge. The table can be thought of as the brain of the agent. Each row in the table represents a state, and the columns represent the possible actions in that state. This will be explained further in the next section.\n",
    "\n",
    "A Q-value is a measure of the total expected reward the agent will receive if it chooses that action and always picks the action with the highest Q-value in the succeeding states, based on its current knowledge of the environment, i.e. the Q-table. Simply said, the higher the Q-value, the better the agent believes the action is.\n",
    "\n",
    "The Q-learning algorithm is used to update the Q-values for a given action $A_{t}$ in the state $S_{t}$, and is updated after each iteration. The equation is shown below.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "A key point to note in this equation is that we are updating the Q(s, a) by using the highest Q-value in the next state, $S_{t+1}$. This value is unknown, and it is therefore just the agent's estimate of that state. This makes it an optimization problem were we gradually adjust each Q-value as we receive a reward moving between the states.\n",
    "\n",
    "\n",
    "This might seem daunting at first, however, we will through this notebook break it into simple pieces, which will hopefully, make it easier to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Q-table\n",
    "The Q-table is the brain of the agent and stores the agent's current knowledge of the Q-values of each state. Each row represents a state, and the columns represent all possible actions in that state. As a result, our table will have the dimensions, (rows=number of states, columns=number of actions). An illustration of the table is shown below, although without the labels on the columns. \n",
    "\n",
    "<img src=\"Images/Taxi_matrix_initial.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "### Task:\n",
    "Create and return a Q-table using numpy, where each cell is initialized to zeros:\n",
    "- Create the Q-table by using the numpy command: np.zeros(shape=[rows, columns]). Read more about it [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
    "- Use the following command to get the number of states in the environment:\n",