{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement learning has resulted in superman performance in a range of scenarios, such as Chess, Atari 2600, and Starcraft, but also in robotics. The main reason for this success compared to traditional Reinforcement Learning is the introduction of deep neural networks. The first successful implementation of deep neural networks in combination with a Reinforcement learning algorithm was Mnih et. al. which was able to reach superhuman performance in the classic Atari games. Their approach was coined Deep Q-Networks(DQN), and uses the Q-learning algorithm we used in the previous notebook, in combination with deep neural networks.\n",
    "\n",
    "In the DQN approach, a neural network replaces the Q-table which we used in the previous notebook. The main reason is the neural networks ability to generalize around similar states. In a continuous environment, a Q-table would quickly grow to tens of millions of unique states, although many of these are so similar that the same action should be applied. This is both an issue due to the huge table we would need, but also because we would need a tremendous amount of training data to update all the states. \n",
    "\n",
    "Neural networks address this issue, but come with their own challenges. Neural network struggles with instabilities during training, in addition to critical forgetting and divergence. Mnih's success was due to several ingenious tricks which stabilized the network during training, and we will go through these during the workshop.\n",
    "\n",
    "In the previous notebook, we created a lot of methods, which resulted in many parameters being passed from method to method. This can quickly become convoluted, and in this notebook we instead use classes. An example can be seen below. When the class is created the init method is run, and the different parameters are set and stored in this instance of the class. The parameters can be called within the class using \"self.<name_of_parameter>\" syntax, e.g. self.learning_rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the notebook. \n",
    "\n",
    "### Task 1\n",
    "Import the packages needed for this workshop, simply mark the cell below and press CTRL + Enter\n",
    "\n",
    "### Task 2\n",
    "Create the necessary support methods by running the second cell below. Mark it and press CTRL + Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import datetime\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from statistics import mean\n",
    "from IPython.display import HTML\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.metrics import MSE\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.initializers import glorot_uniform\n",
    "\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1\n",
    "\n",
    "if not os.path.exists(\"videos\"):\n",
    "    os.makedirs(\"videos\")\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "%precision 3\n",
    "should_assert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blank_weights(layer_dims):\n",
    "    \n",
    "    # Xavier/Glorot Initialization\n",
    "    new_weights = np.random.randn(layer_dims[0], layer_dims[1])*np.sqrt(1/layer_dims[0])\n",
    "    new_bias = np.zeros((layer_dims[1]))\n",
    "    \n",
    "    return [new_weights, new_bias]\n",
    "\n",
    "def create_dummybuffer():\n",
    "    mock_env = gym.make(\"CartPole-v1\")\n",
    "    parameters = {\"buffer_size\": 6,\n",
    "                  \"batch_size\": 3}\n",
    "    DummyBuffer = ExperienceReplay(mock_env, parameters)\n",
    "    \n",
    "    return DummyBuffer\n",
    "\n",
    "def get_dummy_parameters_and_env():\n",
    "        \n",
    "        parameters = {\n",
    "            \"tau\" : 0.4,\n",
    "            \"gamma\" : 1,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 1,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 5,\n",
    "            \"hidden_layer_2\": 5}\n",
    "        \n",
    "        dummy_env = gym.make(\"CartPole-v0\")\n",
    "        return parameters, dummy_env \n",
    "    \n",
    "def clear_video_folder():\n",
    "    video_path = \"videos\"\n",
    "    for item in os.listdir(video_path):\n",
    "        os.remove(os.path.join(video_path, item))\n",
    "\n",
    "def play(agent):\n",
    "    \n",
    "    old_epsilon = agent.epsilon\n",
    "    \n",
    "    done = False\n",
    "    agent.epsilon