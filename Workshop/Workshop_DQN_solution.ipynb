{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement learning has resulted in superman performance in a range of scenarios, such as Chess, Atari 2600, and Starcraft, but also in robotics. The main reason for this success compared to traditional Reinforcement Learning is the introduction of deep neural networks. The first successful implementation of deep neural networks in combination with a Reinforcement learning algorithm was Mnih et. al. which was able to reach superhuman performance in the classic Atari games. Their approach was coined Deep Q-Networks(DQN), and uses the Q-learning algorithm we used in the previous notebook, in combination with deep neural networks.\n",
    "\n",
    "In the DQN approach, a neural network replaces the Q-table which we used in the previous notebook. The main reason is the neural networks ability to generalize around similar states. In a continuous environment, a Q-table would quickly grow to tens of millions of unique states, although many of these are so similar that the same action should be applied. This is both an issue due to the huge table we would need, but also because we would need a tremendous amount of training data to update all the states. \n",
    "\n",
    "Neural networks address this issue, but come w