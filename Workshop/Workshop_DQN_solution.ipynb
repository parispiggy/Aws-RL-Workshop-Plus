{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement learning has resulted in superman performance in a range of scenarios, such as Chess, Atari 2600, and Starcraft, but also in robotics. The main reason for this success compared to traditional Reinforcement Learning is the introduction of deep neural networks. The first successful implementation of deep neural networks in combination with a Reinforcement learning algorithm was Mnih et. al. which was able to reach superhuman performance in the classic Atari games. Their approach was coined Deep Q-Networks(DQN), and uses the Q-learning algorithm we used in the previous notebook, in combination with deep neural networks.\n",
    "\n",
    "In the DQN approach, a neural network replaces the Q-table which we used in the previous notebook. The main reason is the neural networks ability to generalize around similar states. In a continuous environment, a Q-table would quickly grow to tens of millions of unique states, although many of these are so similar that the same action should be applied. This is both an issue due to the huge table we would need, but also because we would need a tremendous amount of training data to update all the states. \n",
    "\n",
    "Neural networks address this issue, but come with their own challenges. Neural network struggles with instabilities during training, in addition to critical forgetting and divergence. Mnih's success was due to several ingenious tricks which stabilized the network during training, and we will go through these during the workshop.\n",
    "\n",
    "In the previous notebook, we created a lot of methods, which resulted in many parameters being passed from method to method. This can quickly become convoluted, and in this notebook we instead use classes. An example can be seen below. When the class is created the init method is run, and the different parameters are set and stored in this instance of the class. The parameters can be called within the class using \"self.<name_of_parameter>\" syntax, e.g. self.learning_rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the notebook. \n",
    "\n",
    "### Task 1\n",
    "Import the packages needed for this workshop, simply mark the cell below and press CTRL + Enter\n",
    "\n",
    "### Task 2\n",
    "Create the necessary support methods by running the second cell below. Mark it and press CTRL + Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import datetime\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from statistics import mean\n",
    "from IPython.display import HTML\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.metrics import MSE\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.initializers import glorot_uniform\n",
    "\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1\n",
    "\n",
    "if not os.path.exists(\"videos\"):\n",
    "    os.makedirs(\"videos\")\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "%precision 3\n",
    "should_assert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blank_weights(layer_dims):\n",
    "    \n",
    "    # Xavier/Glorot Initialization\n",
    "    new_weights = np.random.randn(layer_dims[0], layer_dims[1])*np.sqrt(1/layer_dims[0])\n",
    "    new_bias = np.zeros((layer_dims[1]))\n",
    "    \n",
    "    return [new_weights, new_bias]\n",
    "\n",
    "def create_dummybuffer():\n",
    "    mock_env = gym.make(\"CartPole-v1\")\n",
    "    parameters = {\"buffer_size\": 6,\n",
    "                  \"batch_size\": 3}\n",
    "    DummyBuffer = ExperienceReplay(mock_env, parameters)\n",
    "    \n",
    "    return DummyBuffer\n",
    "\n",
    "def get_dummy_parameters_and_env():\n",
    "        \n",
    "        parameters = {\n",
    "            \"tau\" : 0.4,\n",
    "            \"gamma\" : 1,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 1,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 5,\n",
    "            \"hidden_layer_2\": 5}\n",
    "        \n",
    "        dummy_env = gym.make(\"CartPole-v0\")\n",
    "        return parameters, dummy_env \n",
    "    \n",
    "def clear_video_folder():\n",
    "    video_path = \"videos\"\n",
    "    for item in os.listdir(video_path):\n",
    "        os.remove(os.path.join(video_path, item))\n",
    "\n",
    "def play(agent):\n",
    "    \n",
    "    old_epsilon = agent.epsilon\n",
    "    \n",
    "    done = False\n",
    "    agent.epsilon = 0\n",
    "    total_reward = 0\n",
    "    state = agent.env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action, reward, done, new_state = agent.step(state)\n",
    "        state = new_state\n",
    "        \n",
    "        total_reward += reward\n",
    "    print(\"Total Reward: {}\".format(total_reward))    \n",
    "    \n",
    "    agent.epsilon = old_epsilon\n",
    "    \n",
    "def generate_video(agent):\n",
    "    \n",
    "    if os.listdir(\"./videos/\"):\n",
    "        clear_video_folder()\n",
    "\n",
    "    monitor = gym.wrappers.Monitor(agent.env, directory=\"videos\", force=True)\n",
    "    original_env = agent.env\n",
    "    agent.env = monitor\n",
    "    \n",
    "    old_epsilon = agent.epsilon\n",
    "    \n",
    "    for _ in range(3):\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        agent.epsilon = 0\n",
    "        total_reward = 0\n",
    "        state = agent.env.reset()\n",
    "        while not done:\n",
    "            action, reward, done, new_state = agent.step(state)\n",
    "            state = new_state\n",
    "\n",
    "            total_reward += reward\n",
    "    \n",
    "    print(\"Total Reward: {}\".format(total_reward))    \n",
    "    agent.epsilon = old_epsilon\n",
    "    agent.env = original_env\n",
    "    monitor.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Q-Networks\n",
    "There are several frameworks which can be used to create neural networks such as Tensorflow from Google, PyTorch from Facebook or MXNet from Apache. In this notebook, we will be using Tensorflow, with a high-level API called Keras on top. Keras greatly simplifies the effort needed to create and train a neural network, and it is often referred to as the best Deep Learning framework for those who are just starting with neural networks.\n",
    "\n",
    "Neural networks are partially inspired by the structure of the human brain and consist of a network of interconnected neurons. The neural network consists of an input layer, a set of hidden layers, and an output layer, as shown in the figure below.\n",
    "\n",
    "<img src=\"Images/Neural_network__achitecture.svg\" alt=\"drawing\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Task\n",
    "Now we will experience how easy it is to create a neural network using Keras. We are going to create a neural network with an input layer, two hidden layers, and one output layer. We have completed steps 1,2, 5, and your task is to complete task 3 and 4 according to the design criterions below: <br>\n",
    "\n",
    "1. The model should be fully connected \n",
    "<br>\n",
    "<br>\n",
    "2. The input layer and first hidden layer is created together, and should:\n",
    "    - Be of type Dense\n",
    "    - Use the ReLU activation function\n",
    "    - Have input size equal to the observations size of the environment\n",
    "    - Have hidden layers size equal to parameter hidden_layer_1 \n",
    "<br>\n",
    "<br>\n",
    "3. The second hidden layer should:\n",
    "    - Be of type Dense\n",
    "    - Use the ReLU activation function \n",
    "    - The hidden layer should have size equal to parameter hidden_layer_2\n",
    "    - Tip1: This layer is similar to previous, but without the input_dim parameter\n",
    "    - Tip2: Remember to add the dense layer using model.add()\n",
    "<br>\n",
    "<br>\n",
    "4. The output layer should:\n",
    "    - Be of type Dense\n",
    "    - The activation function should be linear\n",
    "    - Should have size equal to action vector, i.e. action_size\n",
    "<br>\n",
    "<br>\n",
    "5. The final part is to compile the network. Before we do this we must define som parameters\n",
    "    - Loss metric should be Mean-squared-error (MSE)\n",
    "    - The optimizer should be Adaptive Moment Estimation (Adam)\n",
    "    - Learning rate should equal to learning_rate\n",
    "    - Learning rate decay should be equal to learning_rate_decay\n",
    "<br>\n",
    "\n",
    "**Ps!** <br>\n",
    "At the end of the workshop, you can experiment with different structures and parameters, however, in the first walkthrough, you will follow the defined steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.observations_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.learning_rate_decay = parameters[\"learning_rate_decay\"]\n",
    "        self.loss_metric = parameters[\"loss_metric\"]\n",
    "        self.hidden_layer_1 = parameters[\"hidden_layer_1\"]\n",
    "        self.hidden_layer_2 = parameters[\"hidden_layer_2\"]    \n",
    "        \n",
    "    def build_q_network(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_layer_1, input_dim=self.observations_size, activation='relu'))\n",
    "        \n",
    "        \"Input code below\"\n",
    "        model.add(Dense(self.hidden_layer_2, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        \"Input code above\"\n",
    "        \n",
    "        model.compile(loss=self.loss_metric, optimizer=Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
    "    \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if(should_assert):\n",
    "    mock_parameters = {\"loss_metric\" : \"mse\",\n",
    "                \"learning_rate\" : 0.01,\n",
    "                \"learning_rate_decay\": 0.01,\n",
    "                \"hidden_layer_1\": 24,\n",
    "                \"hidden_layer_2\": 24}\n",
    "\n",
    "    mock_env = gym.make(\"CartPole-v0\")\n",
    "    mock_q_network = QNetwork(mock_env, mock_parameters)\n",
    "    mock_model = mock_q_network.build_q_network()\n",
    "    config_network = mock_model.get_config()\n",
    "    assert(config_network.get(\"name\")[0:10] == \"sequential\")\n",
    "\n",
    "    # Layers\n",
    "    config_layers = config_network.get(\"layers\")\n",
    "    assert(len(config_layers) == 3), \"You should only have 2 dense layers\"\n",
    "\n",
    "    # First layer\n",
    "    assert(config_layers[0].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[0].get(\"config\").get(\"batch_input_shape\") == (None, mock_q_network.observations_size))\n",
    "    assert(config_layers[0].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[0].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
    "    \"Activation function for first layer should be relu\"\n",
    "\n",
    "    # Second layer\n",
    "    assert(config_layers[1].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[1].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[1].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
    "    \"Activation function for second layer should be relu\"\n",
    "\n",
    "    # Thrid layer\n",
    "    assert(config_layers[2].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[2].get(\"config\").get(\"units\") == mock_q_network.action_size), \\\n",
    "    \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[2].get(\"config\").get(\"activation\") == \"linear\"),\\\n",
    "    \"Activation function for third layer should be linear\"\n",
    "\n",
    "    config_optimizer = mock_model.optimizer.get_config()\n",
    "    assert(0.0099 < config_optimizer.get(\"lr\") <= 0.01),\"Learning rate should be 0.01\"\n",
    "    assert(0.0099 < config_optimizer.get(\"decay\") <= 0.01),\"Learning rate decay should be 0.01\"\n",
    "    assert(mock_model.loss == \"mse\"), \"Loss metric should me mse\"\n",
    "    \n",
    "    print(\"Superb, you implemented the layers correct! Information about your model is shown below.\\n\")\n",
    "    print(\"PS. The input layer is not shown, altough you should be able to calculate it by looking at <Param #>\")\n",
    "    print(\" of the first hidden layer.\\n\")\n",
    "    mock_model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Experience Buffer\n",
    "\n",
    "For a neural network to perform optimally we want the data to be I.I.D (Independent and Identically Distributed). In supervised learning, where for instance the network is fed an image and it will predict either cat or dog, this is achieved by randomly sampling the training data from the full data set. As a result:\n",
    "- Batches have close-to similar data distribution.\n",
    "- Samples in each batch are independent of each other.\n",
    "\n",
    "In Reinforcement learning where our agent samples data by moving from state to state, the recently sampled data will be highly correlated to each other. As a result, we will feed our network data which closely resembles each other, which is a recipe for disaster when working with a neural network. Furthermore, the data distribution of the initial states will be different from that of the later stage, i.e. this does not satisfy the I.I.D criterion. This is bad news!\n",
    "\n",
    "Luckily Mnih et. al. has a solution. Instead of training on the just the previously collected samples, we store all states in an **Experience Buffer** and sample randomly from this buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "This section is divided into multiple tasks. You will first complete task 1, and then run the first assertion cell for task 1. Then you will continue to task 2 and run assertion cell 2 etc.\n",
    "\n",
    "### Task 1 - Add exp