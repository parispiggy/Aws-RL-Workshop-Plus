
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome \n",
    "Welcome to this workshop!\n",
    "\n",
    "The workshop is structured such that for each cell, you will write your own code and the code will then be asserted in the next cell. The assertion cell is marked \"# Do not edit - Assertion cell #\". <br>\n",
    "The code should be written between \"Write code below\" and \"Write code above\". If your code does not pass the assertion, you will have to rewrite it before continuing.\n",
    "\n",
    "### Task:\n",
    "Implement the method multiply_input_by_2\n",
    "- The code should take the input variabel \"input_variabel\", and multiply it by 2\n",
    "- The answer should be stored in the variabel \"result\"\n",
    "- When the method is implemented, run the cell.\n",
    "\n",
    "Hotkey to run a cell: CTRL + ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(input_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\"\n",
    "print(\"Great, you correctly implemented the method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages\n",
    "To implement the native Q-learning algorithm and use it in an environment, we just need two additional packages.\n",
    "\n",
    "_Numpy_\n",
    "- Numpy adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. We could do this workshop using native Python arrays and built-in methods, however using Numpy simplifies the process. \n",
    "- To call the methods in the package, we use the \"as np\" command. Such that, to create a numpy table with 2 rows and 2 columns where all the cells are zero, you can simply write np.seros(2,2).\n",
    "\n",
    "_OpenAI Gym_\n",
    "- The Gym library is a collection of demo problems an associated environment, that you can use to work out your reinforcement learning algorithms. These range from simple text problems, to complex physical problems, to Atari video games. OpenAI Gym is therefore by many the preferred framework to learn and test Reinforcement learning algorithms.\"\n",
    "\n",
    "\n",
    "### Task:\n",
    "To import the necessary packages, simply run the cell below, and then run the assertion cell to verify that they have been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(gym)\n",
    "    print(\"Great, the packages were imported correctly!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Introduction\n",
    "\n",
    "The environment we are going to use is a simple grid world, where the agent is controlling a taxi. The environment was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop the passenger off in another. You can read more about the environment [here](https://gym.openai.com/envs/Taxi-v2/).\n",
    "\n",
    "### Task:\n",
    "Create the environment variable containing all necessary methods to run the Taxi-v3 game. <br>\n",
    "\n",
    "_Tip: Just run the cell below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_id = \"Taxi-v3\"\n",
    "env = gym.make(environment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(env)\n",
    "    print(\"You successfully created the environement {}\".format(env.spec.id))\n",
    "except:\n",
    "    print(\"You create the environment incorrectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the workshop. \n",
    "\n",
    "### Task\n",
    "Do as you have done before, simply mark the cell below and press CTRL + Enter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.reset()\n",
    "        \n",
    "    def play(self, action):\n",
    "        if not (0 <= int(action) <= 5):\n",
    "            print(\"Action value must either be 0,1,2,3,4 or 5\")\n",
    "            return\n",
    "\n",
    "        clear_output()\n",
    "        _, reward, done, _ = self.env.step(action)\n",
    "        self.env.render()\n",
    "        print(\"Reward: \", reward)\n",
    "        \n",
    "        if(done):\n",
    "            print(\"Game completed, resetting environment!\")\n",
    "            self.reset_env()\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        print(\"Environment has been reset\")\n",
    "        time.sleep(3)\n",
    "        clear_output()\n",
    "\n",
    "class MockData():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.seed(10)\n",
    "        self.should_assert = True\n",
    "        \n",
    "    def get_Q(self):\n",
    "        Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q[0:2,0] = 10\n",
    "        return Q\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "    \n",
    "    def turn_off_assertion(self):\n",
    "        self.should_assert = False\n",
    "        \n",
    "    def do_assertion(self):\n",
    "        return self.should_assert\n",
    "\n",
    "def visualize_q_table():\n",